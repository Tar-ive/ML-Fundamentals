{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tariveadhikari/nn-from-scratch?scriptVersionId=185777583\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt ","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:53:14.852369Z","iopub.execute_input":"2024-06-27T15:53:14.852966Z","iopub.status.idle":"2024-06-27T15:53:14.858558Z","shell.execute_reply.started":"2024-06-27T15:53:14.852925Z","shell.execute_reply":"2024-06-27T15:53:14.857244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:53:28.630491Z","iopub.execute_input":"2024-06-27T15:53:28.630886Z","iopub.status.idle":"2024-06-27T15:53:32.152768Z","shell.execute_reply.started":"2024-06-27T15:53:28.630846Z","shell.execute_reply":"2024-06-27T15:53:32.151723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:53:32.154401Z","iopub.execute_input":"2024-06-27T15:53:32.154734Z","iopub.status.idle":"2024-06-27T15:53:32.193082Z","shell.execute_reply.started":"2024-06-27T15:53:32.154699Z","shell.execute_reply":"2024-06-27T15:53:32.191923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"data = np.array(data)","metadata":{}},{"cell_type":"code","source":"data = np.array(data)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:54:31.20569Z","iopub.execute_input":"2024-06-27T15:54:31.20612Z","iopub.status.idle":"2024-06-27T15:54:31.312334Z","shell.execute_reply.started":"2024-06-27T15:54:31.206089Z","shell.execute_reply":"2024-06-27T15:54:31.311136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:54:43.205035Z","iopub.execute_input":"2024-06-27T15:54:43.205867Z","iopub.status.idle":"2024-06-27T15:54:43.212847Z","shell.execute_reply.started":"2024-06-27T15:54:43.205822Z","shell.execute_reply":"2024-06-27T15:54:43.211651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m,n = data.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:54:59.826274Z","iopub.execute_input":"2024-06-27T15:54:59.826638Z","iopub.status.idle":"2024-06-27T15:54:59.831622Z","shell.execute_reply.started":"2024-06-27T15:54:59.826612Z","shell.execute_reply":"2024-06-27T15:54:59.830484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.shuffle(data)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:55:14.176688Z","iopub.execute_input":"2024-06-27T15:55:14.1771Z","iopub.status.idle":"2024-06-27T15:55:14.725122Z","shell.execute_reply.started":"2024-06-27T15:55:14.177066Z","shell.execute_reply":"2024-06-27T15:55:14.724015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dev = data [0:1000].T\nY_dev= train_dev [0]\nX_dev = train_dev[1:n]\n\ndata_train  = data[1000:m].T\nY_train = data_train[0]\nX_train = data_train[1:n]","metadata":{"execution":{"iopub.status.busy":"2024-06-27T16:30:05.903845Z","iopub.execute_input":"2024-06-27T16:30:05.904243Z","iopub.status.idle":"2024-06-27T16:30:05.909796Z","shell.execute_reply.started":"2024-06-27T16:30:05.904213Z","shell.execute_reply":"2024-06-27T16:30:05.908627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train","metadata":{"execution":{"iopub.status.busy":"2024-06-27T16:30:07.952562Z","iopub.execute_input":"2024-06-27T16:30:07.952944Z","iopub.status.idle":"2024-06-27T16:30:07.960138Z","shell.execute_reply.started":"2024-06-27T16:30:07.952913Z","shell.execute_reply":"2024-06-27T16:30:07.959106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_parms():\n    W1 = np.random.randn(10, 784) - 0.5 \n    b1 = np.random.randn(10, 1) - 0.5 \n    W2 = np.random.randn(10, 10) - 0.5 \n    b2 = np.random.randn(10, 1) - 0.5\n    return W1, b1, W2, b2","metadata":{"execution":{"iopub.status.busy":"2024-06-27T16:30:09.702621Z","iopub.execute_input":"2024-06-27T16:30:09.703029Z","iopub.status.idle":"2024-06-27T16:30:09.708885Z","shell.execute_reply.started":"2024-06-27T16:30:09.702996Z","shell.execute_reply":"2024-06-27T16:30:09.70768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ReLU (Z): \n    return np.maximum(0,Z)\n\ndef softmax(Z): \n    expZ = np.exp(Z - np.max(Z))\n    return expZ / expZ.sum(axis=0, keepdims=True)\n\ndef forward_prop(W1, b1, W2, b2, X):\n    Z1= W1.dot(X)+ b1 \n    A1 = ReLU(Z1)\n    Z2 = W2.dot(A1)+ b2\n    A2 = softmax(A1)\n    return Z1, A1, Z2, A2 \n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T16:30:11.299407Z","iopub.execute_input":"2024-06-27T16:30:11.29978Z","iopub.status.idle":"2024-06-27T16:30:11.306768Z","shell.execute_reply.started":"2024-06-27T16:30:11.29975Z","shell.execute_reply":"2024-06-27T16:30:11.305681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def one_hot(Y): \n    one_hot_Y = np.zeros((Y.size, Y.max()+1))\n    one_hot_Y[np.arange(Y.size),Y]=1\n    one_hot_Y = one_hot_Y.T\n    return one_hot_Y\n\ndef derv_ReLU(Z): \n    return Z>0 \n\ndef back_prop(Z1, A1, Z2, A2,Y): \n    one_hot_Y = one_hot(Y)\n    dZ2 = A2 - one_hot_Y\n    dW2 = 1/m * dZ2.dot(A1.T)\n    db2 = 1/m * np.sum(dZ2 ,2)\n    dZ1 = W2.T.dot(dZ2) * derv_ReLU(Z1)\n    dW1 = 1/m * dZ1.dot(A1.T)\n    db1 = 1/m * np.sum(dZ1 ,2)\n    return dW1, db1, dW2, db2","metadata":{"execution":{"iopub.status.busy":"2024-06-27T16:30:14.509379Z","iopub.execute_input":"2024-06-27T16:30:14.509762Z","iopub.status.idle":"2024-06-27T16:30:14.517684Z","shell.execute_reply.started":"2024-06-27T16:30:14.509732Z","shell.execute_reply":"2024-06-27T16:30:14.516537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def update_parms(dW1, db1, dW2, db2 , W1, b1, W2, b2, alpha): \n    W1 = W1 - alpha * dW1\n    b1 = b1 - alpha *db1 \n    W2 = W2 - alpha* dW2 \n    b2 = b2 - alpha *db2 \n    return W1, b1, W2, b2 ","metadata":{"execution":{"iopub.status.busy":"2024-06-27T16:30:16.606218Z","iopub.execute_input":"2024-06-27T16:30:16.606627Z","iopub.status.idle":"2024-06-27T16:30:16.612954Z","shell.execute_reply.started":"2024-06-27T16:30:16.606592Z","shell.execute_reply":"2024-06-27T16:30:16.611746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_predictions(A2): \n    return np.argmax(A2 , 0)\n\ndef get_accuracy (predictions , Y): \n    print (predictions , Y)\n    return np.sum (predictions == Y)/ Y.size","metadata":{"execution":{"iopub.status.busy":"2024-06-27T16:30:21.554069Z","iopub.execute_input":"2024-06-27T16:30:21.554455Z","iopub.status.idle":"2024-06-27T16:30:21.560275Z","shell.execute_reply.started":"2024-06-27T16:30:21.554426Z","shell.execute_reply":"2024-06-27T16:30:21.558919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gradient_descent(X, Y, iterations, alpha):\n    W1, b1, W2, b2 = init_parms()\n    for i in range(iterations):\n        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n        dW1, db1, dW2, db2 = back_prop(Z1, A1, Z2, A2, X, Y)\n        W1, b1, W2, b2 = update_parms(dW1, db1, dW2, db2, W1, b1, W2, b2, alpha)\n    return W1, b1, W2, b2","metadata":{"execution":{"iopub.status.busy":"2024-06-27T16:30:55.760102Z","iopub.execute_input":"2024-06-27T16:30:55.760499Z","iopub.status.idle":"2024-06-27T16:30:55.767225Z","shell.execute_reply.started":"2024-06-27T16:30:55.76047Z","shell.execute_reply":"2024-06-27T16:30:55.76607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"W1, b1 , W2, b2 = gradient_descent (X_train , Y_train , 500, 0.1)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T16:30:58.220333Z","iopub.execute_input":"2024-06-27T16:30:58.220719Z","iopub.status.idle":"2024-06-27T16:30:58.429405Z","shell.execute_reply.started":"2024-06-27T16:30:58.220688Z","shell.execute_reply":"2024-06-27T16:30:58.427785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef init_parms():\n    np.random.seed(42)  # For reproducibility\n    W1 = np.random.randn(10, 784) * 0.01\n    b1 = np.zeros((10, 1))\n    W2 = np.random.randn(10, 10) * 0.01\n    b2 = np.zeros((10, 1))\n    return W1, b1, W2, b2\n\ndef ReLU(Z):\n    return np.maximum(0, Z)\n\ndef softmax(Z):\n    expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n    return expZ / expZ.sum(axis=0, keepdims=True)\n\ndef forward_prop(W1, b1, W2, b2, X):\n    Z1 = np.dot(W1, X) + b1\n    A1 = ReLU(Z1)\n    Z2 = np.dot(W2, A1) + b2\n    A2 = softmax(Z2)\n    return Z1, A1, Z2, A2\n\ndef compute_cost(A2, Y):\n    m = Y.shape[1]\n    log_probs = -np.log(A2[Y, range(m)])\n    cost = np.sum(log_probs) / m\n    return cost\n\ndef back_prop(Z1, A1, Z2, A2, X, Y):\n    m = X.shape[1]\n    dZ2 = A2\n    dZ2[Y, range(m)] -= 1\n    dZ2 /= m\n\n    dW2 = np.dot(dZ2, A1.T)\n    db2 = np.sum(dZ2, axis=1, keepdims=True)\n\n    dA1 = np.dot(W2.T, dZ2)\n    dZ1 = dA1 * (A1 > 0)\n\n    dW1 = np.dot(dZ1, X.T)\n    db1 = np.sum(dZ1, axis=1, keepdims=True)\n\n    return dW1, db1, dW2, db2\n\ndef update_parms(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n    W1 -= alpha * dW1\n    b1 -= alpha * db1\n    W2 -= alpha * dW2\n    b2 -= alpha * db2\n    return W1, b1, W2, b2\n\ndef gradient_descent(X, Y, iterations, alpha):\n    W1, b1, W2, b2 = init_parms()\n    costs = []\n    for i in range(iterations):\n        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n        cost = compute_cost(A2, Y)\n        costs.append(cost)\n        dW1, db1, dW2, db2 = back_prop(Z1, A1, Z2, A2, X, Y)\n        W1, b1, W2, b2 = update_parms(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n        if i % 100 == 0:\n            print(f\"Cost after iteration {i}: {cost}\")\n    return W1, b1, W2, b2, costs\n\n# Example usage:\n# Assume X_train and Y_train are your data and labels\n# X_train = np.random.randn(784, 41000)\n# Y_train = np.random.randint(10, size=41000)\n# W1, b1, W2, b2, costs = gradient_descent(X_train, Y_train, iterations=1000, alpha=0.01)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T16:31:55.568911Z","iopub.execute_input":"2024-06-27T16:31:55.569328Z","iopub.status.idle":"2024-06-27T16:31:55.585385Z","shell.execute_reply.started":"2024-06-27T16:31:55.569299Z","shell.execute_reply":"2024-06-27T16:31:55.584085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# Initialize parameters\ndef init_parms():\n    np.random.seed(42)  # For reproducibility\n    W1 = np.random.randn(10, 784) * 0.01\n    b1 = np.zeros((10, 1))\n    W2 = np.random.randn(10, 10) * 0.01\n    b2 = np.zeros((10, 1))\n    return W1, b1, W2, b2\n\n# Activation functions\ndef ReLU(Z):\n    return np.maximum(0, Z)\n\ndef softmax(Z):\n    expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n    return expZ / expZ.sum(axis=0, keepdims=True)\n\n# Forward propagation\ndef forward_prop(W1, b1, W2, b2, X):\n    Z1 = np.dot(W1, X) + b1\n    A1 = ReLU(Z1)\n    Z2 = np.dot(W2, A1) + b2\n    A2 = softmax(Z2)\n    return Z1, A1, Z2, A2\n\n# Cost computation\ndef compute_cost(A2, Y):\n    m = Y.shape[0]\n    log_probs = -np.log(A2[Y, range(m)])\n    cost = np.sum(log_probs) / m\n    return cost\n\n# Backpropagation\ndef back_prop(Z1, A1, Z2, A2, X, Y, W2):\n    m = X.shape[1]\n    dZ2 = A2\n    dZ2[Y, range(m)] -= 1\n    dZ2 /= m\n\n    dW2 = np.dot(dZ2, A1.T)\n    db2 = np.sum(dZ2, axis=1, keepdims=True)\n\n    dA1 = np.dot(W2.T, dZ2)\n    dZ1 = dA1 * (A1 > 0)\n\n    dW1 = np.dot(dZ1, X.T)\n    db1 = np.sum(dZ1, axis=1, keepdims=True)\n\n    return dW1, db1, dW2, db2\n\n# Parameter update\ndef update_parms(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n    W1 -= alpha * dW1\n    b1 -= alpha * db1\n    W2 -= alpha * dW2\n    b2 -= alpha * db2\n    return W1, b1, W2, b2\n\n# Gradient descent\ndef gradient_descent(X, Y, iterations, alpha):\n    W1, b1, W2, b2 = init_parms()\n    costs = []\n    for i in range(iterations):\n        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n        cost = compute_cost(A2, Y)\n        costs.append(cost)\n        dW1, db1, dW2, db2 = back_prop(Z1, A1, Z2, A2, X, Y, W2)\n        W1, b1, W2, b2 = update_parms(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n        if i % 100 == 0:\n            print(f\"Cost after iteration {i}: {cost}\")\n    return W1, b1, W2, b2, costs\n\n# Data preparation\ndata = np.random.randn(5000, 785)  # Example data: 5000 samples, 784 features + 1 label\n\n# Splitting data into training and development sets\nm, n_plus_one = data.shape\nn = n_plus_one - 1\n\ntrain_dev = data[0:1000].T\nY_dev = train_dev[0].astype(int)\nX_dev = train_dev[1:n_plus_one]\n\ndata_train = data[1000:m].T\nY_train = data_train[0].astype(int)\nX_train = data_train[1:n_plus_one]\n\n# Training the model\niterations = 1000\nalpha = 0.01\nW1, b1, W2, b2, costs = gradient_descent(X_train, Y_train, iterations, alpha)\n\n# Evaluate on development set\nZ1_dev, A1_dev, Z2_dev, A2_dev = forward_prop(W1, b1, W2, b2, X_dev)\ndev_predictions = np.argmax(A2_dev, axis=0)\n\n# Accuracy on development set\naccuracy = np.mean(dev_predictions == Y_dev)\nprint(f\"Accuracy on development set: {accuracy * 100:.2f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T16:35:38.604379Z","iopub.execute_input":"2024-06-27T16:35:38.604734Z","iopub.status.idle":"2024-06-27T16:36:29.951188Z","shell.execute_reply.started":"2024-06-27T16:35:38.604708Z","shell.execute_reply":"2024-06-27T16:36:29.949918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}